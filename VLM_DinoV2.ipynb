{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267a6345",
      "metadata": {
        "id": "267a6345"
      },
      "outputs": [],
      "source": [
        "from model import Classifier\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b369cb2",
      "metadata": {
        "id": "1b369cb2"
      },
      "source": [
        "### General config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11477bcd",
      "metadata": {
        "id": "11477bcd"
      },
      "outputs": [],
      "source": [
        "# Path to the test image\n",
        "test_image = \"examples/test.jpg\"\n",
        "\n",
        "# Path to the saved model weights\n",
        "model_weights = \"weights/classification_model.pt\"\n",
        "\n",
        "# List of class labels\n",
        "classes = ['Daisy', 'Dandelion', 'Rose', 'Sunflower', 'Tulip']\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(classes)\n",
        "\n",
        "# Device selection (CUDA GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46ad806",
      "metadata": {
        "id": "d46ad806"
      },
      "source": [
        "### Loading the model and pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0124379",
      "metadata": {
        "id": "e0124379",
        "outputId": "759fc826-094d-4a81-e784-8662248abf03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\pthakur/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully...\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the classifier model\n",
        "model = Classifier(num_classes)\n",
        "\n",
        "# Load the pretrained weights into the model\n",
        "model.load_state_dict(torch.load(model_weights))\n",
        "\n",
        "# Set the model to evaluation mode (disable gradient computation)\n",
        "model.eval()\n",
        "\n",
        "# Move the model to the specified device (e.g., GPU)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded successfully...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b1f468",
      "metadata": {
        "id": "a5b1f468"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0772a43c",
      "metadata": {
        "id": "0772a43c"
      },
      "outputs": [],
      "source": [
        "# Define the image transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),  # Randomly crop and resize the image to 224x224 pixels\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.ToTensor(),  # Convert the image to a tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the image tensor\n",
        "])\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess(img_path):\n",
        "    img = Image.open(img_path)  # Open the image file\n",
        "    img = transform(img)  # Apply the defined transformation pipeline to the image\n",
        "    img = img[None, :]  # Add a batch dimension to the image tensor\n",
        "    return img  # Return the preprocessed image tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39109bdc",
      "metadata": {
        "id": "39109bdc"
      },
      "source": [
        "### Load the image and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb259f7",
      "metadata": {
        "id": "ceb259f7"
      },
      "outputs": [],
      "source": [
        "# Preprocess the test image\n",
        "img = preprocess(test_image)\n",
        "\n",
        "# Move the image to the device (e.g., GPU) for computation\n",
        "img = img.to(device)\n",
        "\n",
        "# Pass the preprocessed image through the model to get the result\n",
        "with torch.no_grad():\n",
        "    result = model(img)\n",
        "\n",
        "# Detach the result from the computation graph to avoid backpropagation\n",
        "result = result.detach()\n",
        "\n",
        "# Move the result to the CPU for further processing\n",
        "result = result.cpu()\n",
        "\n",
        "# Convert the result to a numpy array for easier manipulation\n",
        "result = result.numpy()\n",
        "\n",
        "# Find the index of the maximum value in the result array\n",
        "result = np.argmax(result)\n",
        "\n",
        "# Map the index to the corresponding class label\n",
        "result = classes[result]\n",
        "print(\"The output :\", result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dino",
      "language": "python",
      "name": "dino"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}